<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2023-02-23T18:04:32Z</responseDate>
  <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:4003978" metadataPrefix="pmc">https:/www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:pubmedcentral.nih.gov:4003978</identifier>
        <datestamp>2014-04-29</datestamp>
        <setSpec>sensors</setSpec>
        <setSpec>pmc-open</setSpec>
      </header>
      <metadata>
        <article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.3/ https://jats.nlm.nih.gov/archiving/1.3/xsd/JATS-archivearticle1-3.xsd" article-type="research-article">
          <front>
            <journal-meta>
              <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
              <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
              <journal-title-group>
                <journal-title>Sensors (Basel, Switzerland)</journal-title>
              </journal-title-group>
              <issn pub-type="epub">1424-8220</issn>
              <publisher>
                <publisher-name>MDPI</publisher-name>
              </publisher>
            </journal-meta>
            <article-meta>
              <article-id pub-id-type="accession">PMC4003978</article-id>
              <article-id pub-id-type="pmcid">PMC4003978</article-id>
              <article-id pub-id-type="pmc-uid">4003978</article-id>
              <article-id pub-id-type="pmid">24618780</article-id>
              <article-id pub-id-type="pmid">24618780</article-id>
              <article-id pub-id-type="doi">10.3390/s140304981</article-id>
              <article-id pub-id-type="publisher-id">sensors-14-04981</article-id>
              <article-categories>
                <subj-group subj-group-type="heading">
                  <subject>Article</subject>
                </subj-group>
              </article-categories>
              <title-group>
                <article-title>Enhanced Monocular Visual Odometry Integrated with Laser Distance Meter for Astronaut Navigation</article-title>
              </title-group>
              <contrib-group>
                <contrib contrib-type="author">
                  <name>
                    <surname>Wu</surname>
                    <given-names>Kai</given-names>
                  </name>
                  <xref ref-type="aff" rid="af1-sensors-14-04981">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Di</surname>
                    <given-names>Kaichang</given-names>
                  </name>
                  <xref ref-type="aff" rid="af1-sensors-14-04981">
                    <sup>1</sup>
                  </xref>
                  <xref rid="c1-sensors-14-04981" ref-type="corresp">
                    <sup>*</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Sun</surname>
                    <given-names>Xun</given-names>
                  </name>
                  <xref ref-type="aff" rid="af2-sensors-14-04981">
                    <sup>2</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Wan</surname>
                    <given-names>Wenhui</given-names>
                  </name>
                  <xref ref-type="aff" rid="af1-sensors-14-04981">
                    <sup>1</sup>
                  </xref>
                </contrib>
                <contrib contrib-type="author">
                  <name>
                    <surname>Liu</surname>
                    <given-names>Zhaoqin</given-names>
                  </name>
                  <xref ref-type="aff" rid="af1-sensors-14-04981">
                    <sup>1</sup>
                  </xref>
                </contrib>
              </contrib-group>
              <aff id="af1-sensors-14-04981"><label>1</label> State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing 100000, China; E-Mails: <email>wukai@radi.ac.cn</email> (K.W.); <email>wanwh@radi.ac.cn</email> (W.W.); <email>liuzq@radi.ac.cn</email> (Z.L.)</aff>
              <aff id="af2-sensors-14-04981"><label>2</label> Computer and Information Engineering College, Henan University, Kaifeng 475000, China; E-Mail: <email>sunxun2012@163.com</email></aff>
              <author-notes>
                <corresp id="c1-sensors-14-04981"><label>*</label> Author to whom correspondence should be addressed; E-Mail: <email>dikc@radi.ac.cn</email>; Tel.: +86-10-648-68229; Fax: +86-10-648-07987.</corresp>
              </author-notes>
              <pub-date pub-type="collection">
                <month>3</month>
                <year>2014</year>
              </pub-date>
              <pub-date pub-type="epub">
                <day>11</day>
                <month>3</month>
                <year>2014</year>
              </pub-date>
              <volume>14</volume>
              <issue>3</issue>
              <fpage>4981</fpage>
              <lpage>5003</lpage>
              <history>
                <date date-type="received">
                  <day>21</day>
                  <month>1</month>
                  <year>2014</year>
                </date>
                <date date-type="rev-recd">
                  <day>03</day>
                  <month>3</month>
                  <year>2014</year>
                </date>
                <date date-type="accepted">
                  <day>03</day>
                  <month>3</month>
                  <year>2014</year>
                </date>
              </history>
              <permissions>
                <copyright-statement>© 2014 by the authors; licensee MDPI, Basel, Switzerland.</copyright-statement>
                <copyright-year>2014</copyright-year>
                <license>
                  <license-p>This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">http://creativecommons.org/licenses/by/3.0/</ext-link>.</license-p>
                </license>
              </permissions>
              <abstract>
                <p>Visual odometry provides astronauts with accurate knowledge of their position and orientation. Wearable astronaut navigation systems should be simple and compact. Therefore, monocular vision methods are preferred over stereo vision systems, commonly used in mobile robots. However, the projective nature of monocular visual odometry causes a scale ambiguity problem. In this paper, we focus on the integration of a monocular camera with a laser distance meter to solve this problem. The most remarkable advantage of the system is its ability to recover a global trajectory for monocular image sequences by incorporating direct distance measurements. First, we propose a robust and easy-to-use extrinsic calibration method between camera and laser distance meter. Second, we present a navigation scheme that fuses distance measurements with monocular sequences to correct the scale drift. In particular, we explain in detail how to match the projection of the invisible laser pointer on other frames. Our proposed integration architecture is examined using a live dataset collected in a simulated lunar surface environment. The experimental results demonstrate the feasibility and effectiveness of the proposed method.</p>
              </abstract>
              <kwd-group>
                <kwd>monocular visual odometry</kwd>
                <kwd>laser distance meter</kwd>
                <kwd>scale drift</kwd>
                <kwd>calibration</kwd>
                <kwd>navigation</kwd>
              </kwd-group>
            </article-meta>
          </front>
          <body>
            <sec sec-type="intro">
              <label>1.</label>
              <title>Introduction</title>
              <p>The astronaut navigation system is one of the most important systems for manned missions on the lunar surface, as it keeps astronauts safe while exploring previously unknown environments and provides accurate positions for scientific targets. Although the principle of the lunar astronaut navigation system is much the same as that of a pedestrian navigation system, the global positioning system (GPS)-denied environment, and the absence of a dipolar magnetic field and an atmosphere limits the application of several traditional sensors that have been successfully used for pedestrian navigation on Earth, such as GPS, magnetometers and barometers [<xref rid="b1-sensors-14-04981" ref-type="bibr">1</xref>]. Furthermore, unlike lunar or Mars exploration rovers, the size, weight, and power of on-suit astronaut navigation sensors are strictly limited. Therefore, vision sensors are well suited for this type of navigation system, as they are light and power-saving. They can work effectively as long as there are enough textures that can be extracted.</p>
              <p>Visual odometry (VO) is the process of incrementally estimating the pose of an agent from the apparent motion induced on the images of its onboard cameras. Early research into VO was devoted to solving the wheel slippage problem in uneven and rough terrains for planetary rovers; its implementation was finally successfully applied onboard the Mars rovers [<xref rid="b2-sensors-14-04981" ref-type="bibr">2</xref>–<xref rid="b4-sensors-14-04981" ref-type="bibr">4</xref>]. It is fascinating to see that it provides the rover with more accurate positioning compared to wheel odometry. Later Nister [<xref rid="b5-sensors-14-04981" ref-type="bibr">5</xref>] proposed the first long-run VO implementation with a robust outlier rejection scheme. This capability makes it vitally important, especially in GPS-denied environments such as the lunar surface. However, most of the research in VO has been performed using a stereo vision scheme, which is certainly not an optimal vision configuration for an ideal wearable astronaut navigation system, because it is less compact and less power-saving compared to monocular vision. In this case, the stereo vision scheme becomes ineffective and should be substituted by monocular VO. More compact navigation systems [<xref rid="b6-sensors-14-04981" ref-type="bibr">6</xref>] and successful results have been demonstrated using both omnidirectional and perspective cameras [<xref rid="b7-sensors-14-04981" ref-type="bibr">7</xref>,<xref rid="b8-sensors-14-04981" ref-type="bibr">8</xref>]. Closely related to VO is the parallel research undertaken on visual simultaneous localization and mapping (V-SLAM). This aims to estimate both the motion of an agent and the surrounding map. Most V-SLAM work has been limited to small or indoor workspaces [<xref rid="b9-sensors-14-04981" ref-type="bibr">9</xref>,<xref rid="b10-sensors-14-04981" ref-type="bibr">10</xref>] and also involved stereo cameras. This approach is generally not appropriate for large-scale displacements because of algorithmic complexity and growing complexity [<xref rid="b11-sensors-14-04981" ref-type="bibr">11</xref>]. Recently, great developments have been made by Strasdat [<xref rid="b12-sensors-14-04981" ref-type="bibr">12</xref>] using only monocular image input after adopting the key-frame and Bundle Adjustment (BA) [<xref rid="b13-sensors-14-04981" ref-type="bibr">13</xref>] optimization approaches of the state-of-the-art VO systems.</p>
              <p>Due to the nature of monocular systems, with bearing information only available in a single frame, geometry must be inferred over time and 3D landmarks cannot be fully constrained before observations from multiple viewpoints can be made. Furthermore, there is the difficulty that the absolute scale cannot be obtained in a single frame and motion can only be recovered up to a scale factor. This absolute scale cannot be determined unless absolute scale information about the real world is introduced into the system. Without extra measurements, the scale is less constrained and error accumulates over time while motion is integrated from frame-to-frame estimation. This is the scale ambiguity problem for monocular VO. Special attention has been paid to this issue recently and a number of solutions have been proposed to solve the undetermined scale factor. Scaramuzza [<xref rid="b14-sensors-14-04981" ref-type="bibr">14</xref>] used the height of the camera from the ground plane to obtain the global scale factor. Additionally, an observation of the average speed of the vehicle is also proposed to constrain the displacement of the camera [<xref rid="b15-sensors-14-04981" ref-type="bibr">15</xref>]. While these techniques may become popularly used in monocular VO for vehicles, the motion constraints of a steady state may not work out for astronaut navigation. Also, by including additional carefully measured objects in the scene during the initialization stage, such as a calibration object, a metric scale can be fixed [<xref rid="b9-sensors-14-04981" ref-type="bibr">9</xref>]. However, this metric scale is liable to drift over time. The pose-graph optimization technique presented by Strasdat [<xref rid="b12-sensors-14-04981" ref-type="bibr">12</xref>] resolves the scale drift only at loop closures. A commonly used approach called sliding window bundle adjustment has been demonstrated to decrease the scale drift [<xref rid="b16-sensors-14-04981" ref-type="bibr">16</xref>]. In some other work, extra metric sensors, such as inertial measurement units (IMU) and range sensors were also introduced to compensate for scale drift [<xref rid="b17-sensors-14-04981" ref-type="bibr">17</xref>,<xref rid="b18-sensors-14-04981" ref-type="bibr">18</xref>].</p>
              <p>The integration of a camera and a laser distance meter (LDM) was first proposed by Ordonez [<xref rid="b19-sensors-14-04981" ref-type="bibr">19</xref>] and was applied for 2D measurement of façade window apertures. In that work, Ordonez presented in detail the extrinsic calibration method of a digital camera and a LDM. Later, this low-cost 2D measurement system was extended to reconstruct scaled 3D models of buildings [<xref rid="b20-sensors-14-04981" ref-type="bibr">20</xref>].</p>
              <p>The issues mentioned above motivated us to use a monocular camera as the main sensor, aided by LDM for scaled navigation. However, as was admitted by the author in [<xref rid="b20-sensors-14-04981" ref-type="bibr">20</xref>], there is a limitation that the shots must obey a plane constraint and the laser spot of the distance meter must fall in contact with a planar surface. Meanwhile, the process of the extrinsic calibration method of the camera and the laser distance meter proposed above is not simple and robust, as it requires careful intervention from the user, such as manual selection of the laser pointer's projection center.</p>
              <p>In this paper, we focus on the integration of a laser distance meter and a monocular camera for applications such as astronaut navigation. We solve the scale ambiguity problem using metric measurements from a laser distance meter. Nevertheless, unlike 2D laser range finders and 3D laser scanners, which are widely used in the robotics community and provide both range and angular information on a series of laser points, LDM provides only the distance of a single laser dot. Therefore, compared with 2D laser range finders or 3D laser scanners, LDM consumes less power and simplifies the algorithm pipeline when integrated with a camera, as only one pixel of the image contains depth information. Besides, LDM has a more distant range to work in. So far most research concerning integration of a LDM and a camera has been for 3D model measurement or reconstruction, but not for scalable monocular VO. The main contribution of this work is the proposal of a novel enhanced monocular VO scheme by imposing an absolute scale constraint through integrating measurements from the LDM.</p>
              <p>First, to obtain more accurate metric scaled navigation results, a flexible and robust extrinsic calibration method between the camera and the LDM is presented. This whole calibration process requires almost no manual intervention and is robust to gross errors. As soon as extrinsic calibration of the system is completed and geometrical parameters are ready, a global scaled monocular VO pipeline is proposed. We particularly describe how to match the invisible laser spot on other frames in detail and how to correct the scale drift using distance measurement and calibration parameters.</p>
              <p>In principle, this enhanced monocular VO method is certainly applicable for a mobile robot (e.g., a rover). However, stereo vision systems are commonly used in mobile robots due to its less limitation of the size of the navigation payload. In addition to navigation, stereo vision also offers stereo images, which are very valuable for understanding of the surrounding environment and investigation of the interested targets. Thus, in general stereo VO is more favorable than monocular VO for mobile robots.</p>
              <p>This paper is organized as follows: Section 2 gives a general description of the system's device components and the working principle of our global scaled monocular VO scheme. Section 3 and Section 4 present extrinsic calibration and robust motion estimation with LDM and a monocular camera. Section 5 gives expanded results with real outdoor data in a simulated lunar surface environment. Finally, conclusions are given in Section 6.</p>
            </sec>
            <sec>
              <label>2.</label>
              <title>Proposed Approach for Monocular Visual Odometry</title>
              <p>The hardware of the astronaut navigation system consists of five components: an industrial camera (MV-VE141SC/SM, Microvision, Xi'an, China; image dimension: 1392 pixels × 1040 pixels, focal length: 12 mm, max. frequency: 10 Hz), a LDM (CLD-A with RS232 port, Chenglide, Beijing, China; accuracy: ±2 mm, max. frequency: 4 Hz), a specially designed platform for holding these two devices rigidly and provision of power from the on-suit batteries, an industrial computer (CPU: Intel core i5) to control the acquisition of images and laser readings, and an iPad to control the computer triggering the signal to the camera and the distance meter through a local Wi-Fi network. <xref rid="f1-sensors-14-04981" ref-type="fig">Figure 1</xref> shows the hardware components of our navigation system and the right-hand part of the figure shows the screen of the iPad while taking images.</p>
              <p>The laser beam is collinear, so it can be modeled by a straight line. In <xref rid="f2-sensors-14-04981" ref-type="fig">Figure 2</xref>, a view of the navigation system with its mathematical model is shown for clarity. The system calculates the distance <italic>d</italic> between the optical center <italic>O</italic><sub>2</sub> and the laser pointer <italic>P</italic> by knowing the distance <italic>L</italic> from <italic>P</italic> to the LDM's origin <italic>O</italic><sub>1</sub> measured with the LDM, and the geometrical relationship between the camera and the LDM. As we are only interested in the distance <italic>d</italic>, in this paper, the geometrical relationship between the distance meter and the camera is modeled by just two parameters:
<list list-type="bullet"><list-item><p>The distance <italic>B</italic> from the LDM's origin to the optical center of the camera</p></list-item><list-item><p>The direction angle <italic>θ</italic> between the laser beam and the direction from <italic>O</italic><sub>1</sub> to <italic>O</italic><sub>2</sub>.</p></list-item></list></p>
              <p>As illustrated in <xref rid="f2-sensors-14-04981" ref-type="fig">Figure 2</xref>, the following expression can be deduced based on the triangulation principle:
<disp-formula id="FD1"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mi>B</mml:mi><mml:mo>·</mml:mo><mml:mi>L</mml:mi><mml:mo>·</mml:mo><mml:mo>cos</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p>
              <p>These two parameters, <italic>B</italic> and <italic>θ</italic>, are known after extrinsic calibration that will be described in detail in the following section.</p>
              <p>Meanwhile, for each laser pointer reading taken, the point has its projection on its synchronized image which is difficult to detect, as it is mostly invisible on daytime images because the contrast with the environment is too low. Its position can be determined by searching in an index table, which can be created at night by taking a series of images with varying distances and detecting the laser pointer projection center. This index table describes the one-to-one relationship between the distance measured by the LDM and the image position projected on the image. In other words, once we know the distance of the laser pointer measured by the LDM, we can calculate its distance to the camera center and its projection position on the synchronized image taken at the same time. This extra global scale information can be incorporated into monocular VO to restrict scale drift effectively.</p>
              <p><xref rid="f3-sensors-14-04981" ref-type="fig">Figure 3</xref> illustrates the nature of the scale ambiguity of the monocular system and the principle of our enhanced monocular system. The camera position can slide an unknown amount along the translation direction and estimation of the camera pose is intrinsically scale free. However, if we can track the laser pointer <italic>P</italic> on frame <italic>C</italic><sub>n−1</sub> and <italic>C</italic><sub>n+1</sub> successfully, we can obtain its depth at time <italic>t</italic><sub>n</sub> in this stereo model by triangulation. As we can calculate its global scale depth d with <xref rid="FD1" ref-type="disp-formula">Equation (1)</xref> at time <italic>t</italic><sub>n</sub> using the distance measured, we can scale this stereo model with no drift. In this way, the scale drift is corrected whenever tracking along nearby key-frames is successful.</p>
              <p>In this paper, this astronaut navigation system can be divided into two parts: the calibration stage and the navigation stage, as illustrated in <xref rid="f4-sensors-14-04981" ref-type="fig">Figure 4</xref>. As quality of calibration is crucial to ensure accurate estimation of motion displacement, we propose a robust method to implement extrinsic calibration of the camera and the LDM. In particular, we create an index table to establish directly the relationship between the distance measurements from the LDM and its projection position on the synchronized image, further simplifying the calibration process and reducing the systematic error.</p>
              <p>After finishing this preparation stage, the navigation stage is begun using our enhanced VO method. As the quality of image tracking is important for obtaining a robust and accurate estimate [<xref rid="b5-sensors-14-04981" ref-type="bibr">5</xref>,<xref rid="b15-sensors-14-04981" ref-type="bibr">15</xref>], we use the principle that key-points should cover the image as evenly as possible. Image features are tracked along image sequences and only a subset of them, called key-frames, are selected for motion estimation. In previous work, a key-frame selection scheme was proposed and only frames in good state for triangulation were selected [<xref rid="b11-sensors-14-04981" ref-type="bibr">11</xref>]. Our proposal follows this scheme and key-frames are selected for a further core computation step, named motion estimation, and the relative geometrical relationship of the image pair can be constructed. In the meantime, the laser pointer projection is matched on nearby key-frames for triangulation to obtain its relative distance to the camera. In this way, scale drift is corrected when this laser pointer is constrained by the global scale distance calculated from <xref rid="FD1" ref-type="disp-formula">Equation (1)</xref>.</p>
              <p>Most laser pointers are projected on some image position with a weak feature response, thus it is difficult to find its correspondence directly on other images, a coarse-to-fine matching technique is proposed using local disparity constraints followed by dense matching, reducing the possibility of false matches in these local feature-less regions. When the laser pointer is matched successfully, the global scale can be recovered, as illustrated in <xref rid="f3-sensors-14-04981" ref-type="fig">Figure 3</xref>. Otherwise, a relative scale is calculated by exploiting the distance constraints between two image pairs [<xref rid="b21-sensors-14-04981" ref-type="bibr">21</xref>]. By incorporating global scale constraints on monocular VO, we can effectively reduce the scale drift which accumulates quickly over a certain number of frames.</p>
            </sec>
            <sec>
              <label>3.</label>
              <title>Robust Calibration of the Navigation Platform</title>
              <p>The system must be calibrated before it can be used for measurement. Calibration of the system includes camera calibration and extrinsic calibration of the camera and the LDM. First we need to calibrate the camera; we use the commonly used Bouguet method [<xref rid="b22-sensors-14-04981" ref-type="bibr">22</xref>]. A flat panel with grid corners as control points is required. For extrinsic calibration of the camera and the LDM, Ordonez [<xref rid="b19-sensors-14-04981" ref-type="bibr">19</xref>] proposed a calibration method for this combined camera and LDM setup, and the relative orientation of the LDM to the camera is represented as a position vector and an angular unit vector. However, the experiment involves manual selection of the laser pointer projection center and uses only two laser pointer projections, which is not particularly accurate or robust. To increase the accuracy and robustness, we propose a two-step extrinsic calibration method using the principle of the RANSAC scheme. In the following sections, the detailed extrinsic calibration procedure will be introduced.</p>
              <sec>
                <label>3.1.</label>
                <title>Detection of the Laser Pointer Projection Center</title>
                <p>As noted by Ordonez [<xref rid="b19-sensors-14-04981" ref-type="bibr">19</xref>], the precise position of the laser center is defined to be the point that is closest to the starting point. When projected to the synchronized image, this is the center of the brightest region. In our experimental setup, this device is placed in a dark environment, as this facilitates isolation of the laser pointer projection from surrounding environment. The system is initially set facing toward a wall with the measuring axis of the laser meter parallel to the normal ray of the plane surface. The procedure used involves taking a sequence of pictures with synchronized distance measurements while the system is slowly moved toward the wall.</p>
                <p>As we can see from <xref rid="f5-sensors-14-04981" ref-type="fig">Figure 5a,b</xref>, the brightest region changes in size with varying distance, and its boundary is not regular. To detect the laser pointer center effectively, binary classification is first applied to the image, see <xref rid="FD2" ref-type="disp-formula">Equation (2)</xref>:
<disp-formula id="FD2"><label>(2)</label><mml:math id="mm2"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>≤</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></disp-formula>where <italic>t</italic> is the threshold for this binary classification, calculated by maximizing the variances between the classes of pixels below and above the threshold in reference to Otsu's threshold selection method [<xref rid="b23-sensors-14-04981" ref-type="bibr">23</xref>], which works well for images with a bimodal intensity histogram. It's worth noting that quantization noise can be introduced during binarization and may affect the laser pointer center calculation. However, by taking this optimal threshold selection, the effects of binarization can be minimized for detection of the laser pointer center, by making the quantization noise satisfying normal distribution.</p>
                <p>After this binarization, a sequence of operations involving erosion then dilation, known as opening, is performed to isolate a particular component before boundary detection, see <xref rid="FD3" ref-type="disp-formula">Equation (3)</xref>:
<disp-formula id="FD3"><label>(3)</label><mml:math id="mm3"><mml:mrow><mml:mi>I</mml:mi><mml:mo>∘</mml:mo><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>Θ</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⊕</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></disp-formula></p>
                <p>Here we define the structuring element <italic>S</italic> as a 3 × 3 square region, Θ denotes the erosion operation on image <italic>I</italic> while ⊕ denotes the dilation operation on <italic>I</italic>. A concise description of the size, position, and shape of this component is further performed by using moments, which are a rich class of image features describing region size and location. The moments of an image are a scalar defined as <xref rid="FD4" ref-type="disp-formula">Equation (4)</xref>, where (<italic>p</italic> + <italic>q</italic>) is the order of the moment:
<disp-formula id="FD4"><label>(4)</label><mml:math id="mm4"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:msup><mml:mi>v</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mi>I</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
                <p>Here the moments [<xref rid="b24-sensors-14-04981" ref-type="bibr">24</xref>] are given as a physical interpretation by regarding the image function as a mass distribution and the laser pointer projection center is regarded as the center of the total mass of the region calculated as <xref rid="FD5" ref-type="disp-formula">Equation (5)</xref>:
<disp-formula id="FD5"><label>(5)</label><mml:math id="mm5"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><xref rid="f5-sensors-14-04981" ref-type="fig">Figures 5a,b</xref> show the final detection results, with the inner circle overlying the laser pointer projection center. The projection center is precisely located within the illuminated region whether the area is large or small. <xref rid="f6-sensors-14-04981" ref-type="fig">Figure 6</xref> illustrates the relationship between the projected image position of the laser pointer with distance measurements taken synchronously. To increase robustness for fixed distance measurement, multiple observations are made.</p>
                <p>It can be inferred from <xref rid="f6-sensors-14-04981" ref-type="fig">Figure 6</xref> that the laser pointer projection moves with a slower velocity on the image when the distance measured increases gradually, especially when the working distance is greater than 2 m. This characteristic is important because we can directly obtain the image position of the laser pointer from its distance measured through interpolation from a few reference points prepared in a nighttime environment. Fortunately, the working range is generally more than 2 m in our experiment.</p>
              </sec>
              <sec>
                <label>3.2.</label>
                <title>Extrinsic Calibration of the Laser Distance Meter and the Camera</title>
                <p>The above automatic laser pointer projection center detection algorithm produces a correspondence list <italic>S</italic> = {<italic>L<sub>i</sub></italic> ∣(<italic>x<sub>i</sub></italic>,<italic>y<sub>i</sub></italic>, <italic>i</italic> = 1,2,3, …<italic>N</italic>} by associating the distances of many laser points with corresponding synchronized image projection centers. An index table can be created by selecting inliers from <italic>S</italic>. Therefore, for any distance measurement, we can obtain the projection center of the laser pointer on its synchronized image. With this step, we can calculate the extrinsic parameters by taking several shots in front of a grid panel.</p>
                <sec>
                  <label>3.2.1.</label>
                  <title>Laser Pointer Projection Based on the Index Table</title>
                  <p>Theoretically, all the laser pointer projections, although having varying distances, should lie on the same line on the image when the distance meter is rigidly attached to the camera. An exception is that the laser pointers are all projected onto the center of the projection plane regardless of its distance from the camera when the laser beam coincides with the lens axis of an ideal camera. However, this exception can be avoided for a real device configuration. Generally, both camera distortions and errors in the laser pointer center detection contribute to the final offsets of the line. By using the camera distortion coefficients [k<sub>1</sub>,k<sub>2</sub>,k<sub>3</sub>,p<sub>1</sub>,p<sub>2</sub>], the laser pointer image coordinates are all corrected to remove camera distortion effects. Formula <xref rid="FD6" ref-type="disp-formula">(6)</xref> describes these distortion effects:
<disp-formula id="FD6"><label>(6)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>r</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msup><mml:mi>r</mml:mi><mml:mn>6</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>r</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msup><mml:mi>r</mml:mi><mml:mn>6</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>u</mml:mi><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>u</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where (<italic>u<sup>d</sup>,v<sup>d</sup></italic>) is the original image coordinate, (<italic>u,v</italic>) is the image coordinate after distortion offsets are corrected and <italic>r</italic> is the radial distance from the principal point to the image point.</p>
                  <p>The next step is to model this line by using undistorted laser pointer image coordinates projected from a sequence of measurements ranging from far to near. Problems such as this can be transformed into issues of parameter estimation. It can be clearly seen from <xref rid="f7-sensors-14-04981" ref-type="fig">Figure 7a</xref> that these laser pointer image projections are also contaminated by gross errors. We now focus on a line fitting model to introduce the robust estimate techniques.</p>
                  <p>Assuming the blue data set is {(<italic>x<sub>i</sub></italic>,<italic>y<sub>i</sub></italic> ∣ <italic>i</italic> = 1,2,3,…<italic>N</italic>}, for each point we wish to minimize the absolute value of the signed error:
<disp-formula id="FD7"><label>(7)</label><mml:math id="mm7"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
                  <p>Here, the parameter vector <italic>θ</italic> ∈ <italic>R</italic><sup>2</sup> describes the line <italic>ax</italic> + <italic>by</italic> + <italic>c</italic> = 0 (this is the model that we use to fit the measurements). We also model the fitting error as a Gaussian random variable with zero mean and standard deviation <italic>σ<sub>n</sub></italic>, <italic>i.e.</italic>, <italic>e<sub>M</sub></italic> (<italic>d</italic>;<italic>θ</italic>):N(0,<italic>σ<sub>n</sub></italic>). A maximum likelihood approach implemented by MLESAC [<xref rid="b25-sensors-14-04981" ref-type="bibr">25</xref>] (RANSAC's variant) is taken to find the parameter vector [<italic>a</italic>,<italic>b</italic>,<italic>c</italic>] that maximizes the likelihood of the joint error distribution:
<disp-formula id="FD8"><label>(8)</label><mml:math id="mm8"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p>
                  <p>In our implementation, this standard deviation <italic>σ<sub>n</sub></italic> is set to be 0.3 pixels. <xref rid="f7-sensors-14-04981" ref-type="fig">Figure 7b</xref> illustrates the status of the laser pointer projection with outliers removed by running MLESAC. While we obtain an initial estimate of the line's parameter vector, these parameters can be further refined by running nonlinear optimization using all the inliers. In our experiment, the squared mean error is 0.02 pixels for all the inliers after running optimization; thus, an accurate line model is recovered.</p>
                  <p>In <xref rid="f8-sensors-14-04981" ref-type="fig">Figure 8</xref>, supposing that <italic>P</italic> is an inlier, then {<italic>L<sub>i</sub></italic> ∣(<italic>x<sub>i</sub></italic>,<italic>y<sub>i</sub></italic>, <italic>i</italic> = 1,2,3,…<italic>N</italic>} projected from <italic>P</italic> on the refined line is finally chosen as the laser pointer position on the image.</p>
                  <p>An index table <italic>S</italic> = {(<italic>L<sub>i</sub></italic>,<italic>x<sub>i</sub></italic>,<italic>y<sub>i</sub></italic>)∣<italic>i</italic> = 1,2,3,…<italic>N</italic>} is created that directly establishes the relationship between the distance measurements and the laser pointer position on images by using all the inliers. In this way, for any measurement <italic>L</italic> returned by the LDM, we can find its nearby reference points <italic>P<sub>i</sub></italic>(<italic>L<sub>i</sub>,x<sub>i</sub>,y<sub>i</sub></italic>), <italic>P<sub>i</sub></italic><sub>+1</sub>(<italic>L<sub>i</sub></italic><sub>+1</sub><italic>,x<sub>i</sub></italic><sub>+1</sub><italic>,y<sub>i</sub></italic><sub>+1</sub>) from the index table and obtain its associated projection <italic>P</italic>(<italic>x</italic>,<italic>y</italic>) by interpolating from <italic>P<sub>i</sub></italic>, <italic>P<sub>i</sub></italic><sub>+1</sub>, thus:
<disp-formula id="FD9"><label>(9)</label><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>•</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>•</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>•</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>•</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
                  <p>As we can see from <xref rid="FD9" ref-type="disp-formula">Equation (9)</xref>, the more densely sampled our reference points are, the more precise our interpolated image position will be. In our experimental setup, because the navigation platform is moved slowly toward the wall, for any pair of nearby reference points <italic>P<sub>i</sub></italic>(<italic>L<sub>i</sub>,x<sub>i</sub>,y<sub>i</sub></italic>),<italic>P<sub>i</sub></italic><sub>+1</sub>(<italic>L<sub>i</sub></italic><sub>+1</sub><italic>,x<sub>i</sub></italic><sub>+1</sub><italic>,y<sub>i</sub></italic><sub>+1</sub>) the constraints <xref rid="FD10" ref-type="disp-formula">(10)</xref> are maintained, which keeps the interpolation error within one pixel:
<disp-formula id="FD10"><label>(10)</label><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
                </sec>
                <sec>
                  <label>3.2.2.</label>
                  <title>Geometrical Calibration of LDM and Camera</title>
                  <p>A flat panel with grid corners is all that is necessary for the geometrical calibration. We take images from different positions on the panel with synchronized measurements from the LDM. For brevity, in the following account, each exposure and its associated measurement is referred to as a “shot” [<xref rid="b18-sensors-14-04981" ref-type="bibr">18</xref>]. The calibration sequence and the calculation of geometrical parameters <italic>B</italic> and <italic>θ</italic> are summarized as follows:</p>
                  <p>A reference system is defined in the panel such that regular grid corners are assigned coordinates (see <xref rid="f9-sensors-14-04981" ref-type="fig">Figure 9</xref>). We assume the panel to be completely flat; all the reference points have the same <italic>Z</italic> coordinate of 0.
<list list-type="simple"><list-item><label>(1)</label><p>For every shot, we need to know the camera coordinates <italic>T</italic>(<italic>X<sub>o</sub>,Y<sub>o</sub>,Z<sub>o</sub></italic>) and its rotations <italic>R</italic>(<italic>φ<sub>o</sub></italic>,<italic>ω<sub>o</sub></italic>,<italic>κ<sub>o</sub></italic>) with reference to the panel. As the camera parameters have been calibrated, we use the same calibration package [<xref rid="b22-sensors-14-04981" ref-type="bibr">22</xref>] to detect the grid corners and then compute extrinsic parameters only. An estimate of uncertainty of these parameters can also be obtained.</p><p>From the index table created above, the projection of the laser pointer <italic>p</italic>(<italic>x<sub>p</sub></italic>,<italic>y<sub>p</sub></italic>) is also known from its synchronized image calculated using <xref rid="FD11" ref-type="disp-formula">Equation (11)</xref>. In <xref rid="f9-sensors-14-04981" ref-type="fig">Figure 9</xref>, the vector 
<inline-formula><mml:math id="mm11"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> can be described by the following equation:
<disp-formula id="FD11"><label>(11)</label><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mo>sin</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>sin</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>sin</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mo>sin</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mo>sin</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>sin</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>cos</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>•</mml:mo><mml:msup><mml:mi>κ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>•</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic>K</italic><sup>−1</sup> is the inverse of the camera intrinsic matrix. By intersecting 
<inline-formula><mml:math id="mm13"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> with the panel, we obtain the laser pointer <italic>P</italic>(<italic>X<sub>p</sub>,Y<sub>p</sub>,Z<sub>p</sub></italic>).The distance <italic>d</italic> can then be calculated:
<disp-formula id="FD12"><label>(12)</label><mml:math id="mm14"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>O</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p></list-item><list-item><label>(2)</label><p>From <xref rid="FD1" ref-type="disp-formula">Equation (1)</xref>, it is clear that we require only two shots to compute geometrical parameters theoretically. For example, for any two shots with laser measurements <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub>, we can calculate the corresponding laser pointer distances <italic>d</italic><sub>1</sub>, <italic>d</italic><sub>2</sub> to the camera center following the steps above. The geometrical parameters can then be calculated:
<disp-formula id="FD13"><label>(13)</label><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>•</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>•</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>cos</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>•</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>•</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>However, instead of taking all the shots into consideration, we choose shots providing good data by running a RANSAC scheme. Here, the error function <italic>f</italic>(<italic>L</italic>,<italic>d</italic>) for every laser pointer <italic>j</italic> is defined to be:
<disp-formula id="FD14"><label>(14)</label><mml:math id="mm16"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>•</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>•</mml:mo><mml:mi>B</mml:mi><mml:mo>•</mml:mo><mml:mo>cos</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></disp-formula></p><p>In our experiment, the error threshold is set to be 1 cm and error values below this threshold are grouped as inliers. Using this threshold setting, <italic>B</italic> and <italic>θ</italic> can be recovered robustly, rejecting those shots with great uncertainty.</p></list-item><list-item><label>(3)</label><p>Step 3 provides us with initial values for the extrinsic parameters and the camera shots that provide good data. These parameters are further refined by minimizing the error calculated from <xref rid="FD14" ref-type="disp-formula">Equation (14)</xref>, using the Levenberg-Marquardt method. Generally, convergence is reached within three to four iterations and the squared mean error is 2 mm, nearly the same accuracy as that of the LDM.</p></list-item></list></p>
                </sec>
              </sec>
            </sec>
            <sec>
              <label>4.</label>
              <title>A Global Scaled Monocular VO Algorithm for Astronaut Navigation</title>
              <p>Once all the calibration steps mentioned above are completed, we are ready for astronaut navigation. This navigation framework, as illustrated in <xref rid="f4-sensors-14-04981" ref-type="fig">Figure 4</xref>, can be seen as an extension to traditional monocular VO. We will present the algorithm in three parts. The first part deals with robust motion estimation, while the second part deals with laser pointer tracking on image. The final part deals with the scale ambiguity problem, including relative scale calculation and global scale correction with the aid of laser measurements.</p>
              <sec>
                <label>4.1.</label>
                <title>Robust Relative Motion Estimation</title>
                <p>The core of VO is robust estimation of frame-to-frame motion. This is a classical problem, and a large number of solutions have been proposed to solve this issue [<xref rid="b26-sensors-14-04981" ref-type="bibr">26</xref>]. Most of these works use RANSAC for outlier rejection. In our work, we choose to integrate a variant of RANSAC, named PROSAC [<xref rid="b27-sensors-14-04981" ref-type="bibr">27</xref>], to remove outliers more effectively. As is reported by Chum [<xref rid="b27-sensors-14-04981" ref-type="bibr">27</xref>], PROSAC exploits the linear ordering defined on the correspondences by a similarity function instead of treating all correspondences equally. In our experiments, it can detect outliers more effectively than RANSAC, while achieving large computational savings. The relative motion is then computed by 2D-to-2D motion estimation. Iterative refinement is also performed to obtain more accurate results by using all the inliers. We summarize the steps in the robust motion estimation algorithm as follows:
<list list-type="simple"><list-item><label>(1)</label><p>Image features are detected using the Shi-Tomasi corner detector [<xref rid="b28-sensors-14-04981" ref-type="bibr">28</xref>,<xref rid="b29-sensors-14-04981" ref-type="bibr">29</xref>]. These features are further refined to reach sub-pixel localization accuracy [<xref rid="b29-sensors-14-04981" ref-type="bibr">29</xref>,<xref rid="b30-sensors-14-04981" ref-type="bibr">30</xref>]. As the images are taken from nearby viewpoints, instead of detecting features individually in images and then matching them, we find features in one image and track these features in the succeeding images by searching in the nearby neighborhood. We take an implementation called the Kanade-Lucas-Tomasi (KLT) tracker [<xref rid="b31-sensors-14-04981" ref-type="bibr">31</xref>] to track features over long image sequences. Mutual consistency checking is also undertaken to remove false matches. As the distribution of features has been reported to affect VO results [<xref rid="b5-sensors-14-04981" ref-type="bibr">5</xref>,<xref rid="b8-sensors-14-04981" ref-type="bibr">8</xref>], the image is portioned into 10 buckets by 10 buckets and the detector is applied to each cell with a threshold of a maximum number of features set in each bucket.</p></list-item><list-item><label>(2)</label><p>Key-frames are selected automatically based on the number of features tracked. A criterion is set up such that a new key-frame <italic>I<sub>i</sub></italic> is introduced whenever the number of stereo matches with the last key-frame <italic>I<sub>i</sub></italic><sub>+1</sub> is below <italic>M</italic><sub>1</sub> [<xref rid="b11-sensors-14-04981" ref-type="bibr">11</xref>]. Additionally, a key-frame is introduced whenever the number of triple matches with key-frame <italic>I<sub>i</sub></italic><sub>−2</sub> is below <italic>M</italic><sub>2</sub> (in our experiment, we set <italic>M</italic><sub>1</sub> = 1000 and <italic>M</italic><sub>2</sub> = 300), as shown in <xref rid="f10-sensors-14-04981" ref-type="fig">Figure 10</xref>. After this key-frame is selected, mutual consistency checking is performed again between <italic>I<sub>i</sub></italic> and <italic>I<sub>i</sub></italic><sub>+1</sub> in case of tracking drift issues [<xref rid="b32-sensors-14-04981" ref-type="bibr">32</xref>].</p></list-item><list-item><label>(3)</label><p>The essential matrix between frame <italic>I<sub>i</sub></italic> and <italic>I<sub>i</sub></italic><sub>−1</sub> is estimated using the 5-point algorithm [<xref rid="b33-sensors-14-04981" ref-type="bibr">33</xref>] and PROSAC followed by matrix factorization into rotation <italic>R</italic> and unitary translation <italic>T</italic> using Horn's method [<xref rid="b34-sensors-14-04981" ref-type="bibr">34</xref>].</p></list-item><list-item><label>(4)</label><p>The refinement of <italic>R</italic> and <italic>T</italic> is further refined by minimizing the reprojection error using the Levenberg-Marquardt nonlinear optimization.</p></list-item></list></p>
                <p>Currently, our motion estimation is simple for the selection of key-frames, by making the assumption that the distribution of features is in a good state in the image and the number of features tracked from the last key-frame falls off gradually when subsequent frames arrive. Therefore, a threshold can be set to restrict the track length as in step 2. In particular, the number of triple matches involving the last two key-frames is constrained for further calculation of relative scale between nearby image pairs. However, this assumption fails when rapid cornering occurs, which is not taken into consideration in our current VO scheme and was avoided in the experiment.</p>
              </sec>
              <sec>
                <label>4.2.</label>
                <title>Laser Pointer Matching</title>
                <p>Suppose that at time <italic>t<sub>n</sub></italic> during the navigation stage, we obtain the laser pointer <italic>P</italic>'s distance measurement <italic>L</italic> and synchronized image frame <italic>C<sub>n</sub></italic>. We can easily obtain the projection of the laser pointer <italic>p</italic> on <italic>C<sub>n</sub></italic> from the index table created during calibration. As previously mentioned, we need to match <italic>p</italic> in the previous key-frame and the next key-frame to correct scale drift. The difficulty lies in that the projection often does not belong to feature points that can be detected by feature detectors, and it is difficult to match <italic>p</italic> in other frames with commonly used matching techniques. In our work, a coarse-to-fine matching method is proposed to deal with this difficulty. Epipolar constraints are also taken into consideration for robustness. The principle of this coarse-to-fine matching method is as follows:
<list list-type="simple"><list-item><label>(1)</label><p>A window centered on <italic>p</italic> with a radius of 50 pixels is used to select feature points located in this window from frame <italic>C<sub>n</sub></italic>. Here, we denote this sub-image as <italic>I</italic><sub>1</sub>. As the correspondences of these feature points with the nearby key-frame are known during feature tracking, we can compute the disparity of these feature point with reference to this key-frame. A Delaunay triangulation is constructed for this set of selected feature points. By searching in the Delaunay triangulation, a triangle containing <italic>p</italic> can be found. For the three vertex points of the triangle, we establish the affine transformation from the coordinates of these three points in frame <italic>C<sub>n</sub></italic> to the nearby key-frame. By solving the affine transformation parameters, we can obtain <italic>p</italic>'s transformed coordinate, <italic>p</italic><sub>1</sub> in the nearby key-frame.</p></list-item><list-item><label>(2)</label><p>A window centered on <italic>p</italic><sub>1</sub> with a radius of 50 pixels is constructed in the nearby key-frame. We denote this sub-image as <italic>I</italic><sub>2</sub>. More feature points are detected by lowering the response threshold of the feature in <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub>. Thus, dense feature points can be matched, followed by RANSAC to remove outliers. A similar step is performed by constructing a new Delaunay triangulation to calculate <italic>p</italic>'s transformed coordinate, <italic>p</italic><sub>2</sub>. By using constraints with dense feature points, <italic>p</italic><sub>2</sub> becomes closer to the laser pointer projection in the nearby key-frame.</p></list-item><list-item><label>(3)</label><p>These steps help us to find a good initial position. Then we can use dense tracking techniques to calculate the final position with further refinement using Horn's method [<xref rid="b35-sensors-14-04981" ref-type="bibr">35</xref>]. In our experiment, we set the search range to be 3 pixels, which greatly decreases false matches by constraining the search range to within a small area.</p></list-item><list-item><label>(4)</label><p>Now we obtain the refined image position on the nearby key-frame after performing the last three steps successfully. To increase the robustness of this coarse-to-fine method, the final refined position on the key-frame is verified using epipolar constraints performed as below:</p></list-item></list></p>
                <p>For the laser pointer projection in frame <italic>C<sub>n</sub></italic>, we can calculate its corresponding epipolar line on the nearby key-frame. This final position is accepted as valid when the distance to this epipolar line is within one pixel, as shown in <xref rid="f11-sensors-14-04981" ref-type="fig">Figure 11</xref>.</p>
                <p>The principle of the coarse-to-fine matching scheme is robust when choosing a good initial position as close as possible to the laser pointer's projected position on the nearby key-frame, thus decreasing the possibility of choosing false matches that also have similar image patches. Then subsequent local dense matching techniques can be performed effectively within a small area.</p>
              </sec>
              <sec>
                <label>4.3.</label>
                <title>Robust Scale Estimation</title>
                <p>Although we obtain the transformation relationship <italic>T<sub>i</sub></italic><sub>,</sub><italic><sub>i</sub></italic><sub>+1</sub> between the image pair {<italic>i</italic>,<italic>i</italic>+1} through motion estimation, we need to concatenate <italic>T<sub>i</sub></italic><sub>,</sub><italic><sub>i</sub></italic><sub>+1</sub> with the previous transformation <italic>T<sub>i</sub></italic><sub>−</sub><italic><sub>1</sub></italic><sub>,</sub><italic><sub>i</sub></italic> estimated from image pair {<italic>i</italic>−<italic>1</italic>,<italic>i</italic>} to recover the trajectory of the whole image sequence. When laser pointer projection is successfully matched, we can scale this relative model using the global distance. Otherwise, we use the relative scale calculation with the triple match constraint.</p>
                <sec>
                  <label>4.3.1.</label>
                  <title>Computation of Relative Scale</title>
                  <p>For monocular image sequences, a proper relative scale must be calculated when the absolute scale of the transformation cannot be computed without extra absolute scale information. Triple matches across three key-frames are required to calculate this relative scale [<xref rid="b36-sensors-14-04981" ref-type="bibr">36</xref>]. One simple way of doing this [<xref rid="b37-sensors-14-04981" ref-type="bibr">37</xref>] is to triangulate two 3-D points, <italic>X<sub>m</sub></italic> and <italic>X<sub>n</sub></italic>, from image pairs {<italic>i</italic>,<italic>i</italic>+1} and {<italic>i</italic>−<italic>1</italic>,<italic>i</italic>}, then the relative scale can be determined from the distance ratio between point pairs in subsequent image pairs as follows:
<disp-formula id="FD15"><label>(15)</label><mml:math id="mm17"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
                  <p>We can see from <xref rid="FD15" ref-type="disp-formula">Equation (15)</xref> that at least two features need to be matched across three frames. For robustness, scale ratios for many point pairs are computed and the median values are often chosen in case of outliers. In our experiment, nearly 300 features across three frames are kept for this relative scale calculation, which is thus robust against gross errors.</p>
                </sec>
                <sec>
                  <label>4.3.2.</label>
                  <title>Correction of Scale Drift with Global Scale Constraints</title>
                  <p>As monocular VO works by computing the camera route incrementally, the path errors are bound to grow over time with the accumulation of frame-to-frame motion. In our navigation task, it is important to keep path drift as small as possible. Though bundle-adjustment-based monocular VO has been proved to decrease the path drift effectively, it still suffers from scale drift. In our work, we concentrated on the correction of scale drift through combination with a laser distance meter. We have given a brief note on the principle of our method in <xref rid="f3-sensors-14-04981" ref-type="fig">Figure 3</xref>. The detailed steps in the correction of the scale drift are summarized as follows:
<list list-type="simple"><list-item><label>(1)</label><p>When a new key-frame is introduced, laser pointer tracking is performed on this new key-frame and its previous key-frame.</p></list-item><list-item><label>(2)</label><p>If laser pointer matching fails, we return to the traditional relative scale calculation. Otherwise, supposing that this laser pointer is collected at time <italic>t<sub>n</sub></italic> with laser meter measurement <italic>L<sub>n</sub></italic>, synchronized frame <italic>C<sub>n</sub></italic> and nearby key-frame pairs <italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub>, we obtain the projection of the laser pointer <italic>p<sub>n</sub></italic> on <italic>C<sub>n</sub></italic> using the index table and the projections <italic>p</italic><sub>1</sub> on <italic>C</italic><sub>1</sub> and <italic>p</italic><sub>2</sub> on frame <italic>C</italic><sub>2</sub> through laser pointer matching. As we obtain the transformation relationship <italic>T</italic> using the frame pairs <italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub>, we can obtain the 3-D position of the laser pointer <italic>P<sub>n</sub></italic> by triangulating <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>. When multiple laser pointers are matched successfully, we select the one with the maximum intersection angle. Most of the time, <italic>C<sub>n</sub></italic> is between key-frame pairs, as illustrated in <xref rid="f3-sensors-14-04981" ref-type="fig">Figure 3</xref>. However, when it is exactly the new key-frame, we only need to match the laser pointer on the previous key-frame.</p></list-item><list-item><label>(3)</label><p>By triangulating image pairs between <italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub>, we can obtain a series of 3-D points. As we also know the projection of the images of these 3-D points on <italic>C<sub>n</sub></italic>, we can obtain the position of the camera <italic>P<sub>c</sub></italic> at time <italic>t<sub>n</sub></italic> by solving the PnP (Pose from n Points) problem [<xref rid="b37-sensors-14-04981" ref-type="bibr">37</xref>]. Meanwhile, the global distance <italic>d<sub>n</sub></italic> from the laser pointer to the centre of the camera at time <italic>t<sub>n</sub></italic> can be calculated from <xref rid="FD1" ref-type="disp-formula">Equation (1)</xref> with <italic>L<sub>n</sub></italic> and calibrated geometrical parameters. Thus, we can calculate the global scale as follows:
<disp-formula id="FD16"><label>(16)</label><mml:math id="mm18"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>We can see from the above that scale drift can be corrected whenever the laser pointer is successfully matched on the key-frame pair. When laser pointer matching fails, we can use relative scale calculation, introducing scale drift when these relative scales accumulate until the next laser point is matched again. In this way, our enhanced monocular VO corrects the scale drift over a certain number of frames, effectively reducing the final position drift.</p></list-item></list></p>
                </sec>
              </sec>
            </sec>
            <sec>
              <label>5.</label>
              <title>Experiments and Results</title>
              <p>In this section, field tests using the proposed enhanced monocular VO algorithm are carried out in simulated lunar environments. These tests are designed to validate the feasibility and effectiveness of the proposed enhanced monocular VO method. The camera we used has a field of view of 30° and is rigidly attached to a LDM while facing forward during walking. The laser frequency was set at 1 Hz and the camera was set at 10 Hz, meaning that we have the distance measurements with synchronized captured images every second. The person carrying the system walked at a velocity of about 1.0 m/s. Here we report two typical experiments on outdoor and simulated lunar environments.</p>
              <p>For the first dataset (see <xref rid="f12-sensors-14-04981" ref-type="fig">Figure 12a</xref>), 1108 frames and 89 laser pointer measurements (when the distance signal does not return in a valid time, this measurement is dropped in our system) were taken at a construction site covering a total distance of approximately 110 m. Finally, 173 key-frames were selected automatically and 41 key-frame pairs were successfully matched with the laser pointer projection. We can see from <xref rid="f12-sensors-14-04981" ref-type="fig">Figure 12a</xref> that the soil hardness is not as soft as that of the lunar surface and rocks of various sizes are spread across the terrain surface, which make features quite easy to detect among the two datasets. Therefore the first dataset can be taken as an ideal dataset to evaluate the performance of proposed VO method compared to traditional one. The second dataset was taken in a desert which seems to be more similar to the lunar surface. For the second dataset, there are 3840 frames and 368 laser pointer measurements covering a total distance of 300 m, of which 402 key-frames were selected and 154 key-frame pairs found laser pointer's projection. We can see from <xref rid="f12-sensors-14-04981" ref-type="fig">Figure 12b</xref> that the sandy surface in this dataset is more similar to the type of the lunar surface than the former dataset, with footprints clearly seen on this soft sandy terrain. Therefore, we can take it as a large-scale outdoor test field to simulate the real lunar terrain.</p>
              <p>We walked a loop with the origin set at [0,0]. The same image was used for the first and last positions to ensure that the true last camera pose was exactly the same as where the first image was recorded. The commonly used approach called sliding window bundle adjustment is not involved in our current monocular VO scheme, nor is the loop closure correction.</p>
              <p>Given that the loop is closed, we can use it to measure the navigation accuracy. <xref rid="f13-sensors-14-04981" ref-type="fig">Figure 13</xref> shows the final result of our enhanced monocular VO scheme compared with the traditional scheme. The motion estimates of the first key-frame pair are both globally scaled with the LDM to facilitate comparison. From <xref rid="f13-sensors-14-04981" ref-type="fig">Figure 13a</xref>, obvious improvement can be seen with the relative error decreased from 5.91% to 0.54% for the first dataset when the LDM is added. In <xref rid="f13-sensors-14-04981" ref-type="fig">Figure 13b</xref>, the relative error decreased from 12.02% to 1.71% for the 300 m route in the desert. Considering the second dataset's high similarity to the lunar surface, it can be inferred that this enhanced monocular VO scheme should also work well when dealing with real lunar environment. Moreover, it can be inferred from <xref rid="f13-sensors-14-04981" ref-type="fig">Figure 13b</xref> the longer we walked, the higher the improvement is as scale drift accumulates severely for a single camera. As we emphasize the scale drift issue, the relative transformation relationships of key-frames are kept the same for the enhanced VO and the traditional VO except for the difference in scale selection during the whole trajectory, which shows that the accuracy is improved significantly by a better scale selection scheme.</p>
              <p>We also compared the distance errors of laser points between LDM-aided monocular VO and the traditional one with the distance travelled. By taking the distance calculated from <xref rid="FD1" ref-type="disp-formula">Equation (1)</xref> as a reference, we triangulate the laser pointer's projections in key-frame pairs in both VO schemes, as illustrated in <xref rid="f14-sensors-14-04981" ref-type="fig">Figure 14</xref>. As the distance constraints of the first laser pointer are used in both VO schemes, this error is set as zero in the beginning. It is clear from <xref rid="f14-sensors-14-04981" ref-type="fig">Figure 14</xref> that the gradually accumulated scale drift is corrected effectively with our VO scheme.</p>
            </sec>
            <sec>
              <label>6.</label>
              <title>Summary and Conclusions</title>
              <p>In this paper, we have presented an enhanced monocular VO scheme to resolve the scale drift with the aid of LDM. We concentrated on the integration of LDM with monocular camera mounted on a walking person modeling astronaut navigation on a simulated lunar surface. A robust and simple extrinsic calibration method has been proposed. Based on this method, for every laser point measured, its projected image position and the distance to synchronized camera center is also precisely known. Later, an enhanced monocular VO scheme was proposed by integrating measurements from LDM. Accurate results for approximately 110 m of walking at a construction site were demonstrated by correcting the scale drift, outperforming the traditional monocular scheme by almost a factor of ten. Further experiments were taken in a desert to validate our method's feasibility and robustness on simulated lunar terrain compared to traditional one.</p>
              <p>One of the most remarkable differences between our monocular VO scheme and previous methods is the introduction of LDM to correct the scale drift. Scale error propagation over time is avoided effectively, demonstrating the strength of LDM in the field of monocular VO. In our current system, the commonly used BA technique is not used. In the future, the sliding window BA with measurements using LDM will be integrated into our system, further improving the pose drift and being more practicable for astronaut long term navigation.</p>
            </sec>
          </body>
          <back>
            <ack>
              <p>This research is funded by the National Natural Science Foundation of China (41171355) and National Basic Research Program of China (2012CB719902).</p>
            </ack>
            <notes>
              <title>Author Contributions</title>
              <p>Kaichang Di conceived the research and designed the overall methodology. Kai Wu designed and developed the algorithms and performed the experiments. Xun Sun constructed the hardware components. Wenhui Wan and Zhaoqin Liu participated in field data collection and processing.</p>
            </notes>
            <notes>
              <title>Conflicts of Interest</title>
              <p>The authors declare no conflict of interest.</p>
            </notes>
            <ref-list>
              <title>References</title>
              <ref id="b1-sensors-14-04981">
                <label>1.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>He</surname>
                      <given-names>S.</given-names>
                    </name>
                  </person-group>
                  <article-title>Integration of Multiple Sensors for Astronaut Navigation on The Lunar Surface</article-title>
                  <source>Ph.D. Thesis</source>
                  <publisher-name>The Ohio State University</publisher-name>
                  <publisher-loc>Columbus, OH, USA</publisher-loc>
                  <year>2012</year>
                  <fpage>1</fpage>
                  <lpage>8</lpage>
                </element-citation>
              </ref>
              <ref id="b2-sensors-14-04981">
                <label>2.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Maimone</surname>
                      <given-names>M.W.</given-names>
                    </name>
                    <name>
                      <surname>Cheng</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Matthies</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Two years of visual odometry on the mars exploration rovers</article-title>
                  <source>J. Field. Rob.</source>
                  <year>2007</year>
                  <volume>24</volume>
                  <fpage>169</fpage>
                  <lpage>186</lpage>
                </element-citation>
              </ref>
              <ref id="b3-sensors-14-04981">
                <label>3.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cheng</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Maimone</surname>
                      <given-names>M.W.</given-names>
                    </name>
                    <name>
                      <surname>Matthies</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual odometry on the Mars exploration rovers—A tool to ensure accurate driving and science imaging</article-title>
                  <source>IEEE Rob. Autom. Mag.</source>
                  <year>2006</year>
                  <volume>13</volume>
                  <fpage>54</fpage>
                  <lpage>62</lpage>
                </element-citation>
              </ref>
              <ref id="b4-sensors-14-04981">
                <label>4.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Cheng</surname>
                      <given-names>Y.</given-names>
                    </name>
                    <name>
                      <surname>Maimone</surname>
                      <given-names>M.W.</given-names>
                    </name>
                    <name>
                      <surname>Matthies</surname>
                      <given-names>L.</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual odometry on the Mars exploration rovers</article-title>
                  <conf-name>Proceedings of the 2005 IEEE International Conference on Systems, Man and Cybernetics</conf-name>
                  <conf-loc>Hawaii, HI, USA</conf-loc>
                  <conf-date>10–12 October 2005</conf-date>
                  <fpage>903</fpage>
                  <lpage>910</lpage>
                </element-citation>
              </ref>
              <ref id="b5-sensors-14-04981">
                <label>5.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nistér</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Naroditsky</surname>
                      <given-names>O.</given-names>
                    </name>
                    <name>
                      <surname>Bergen</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual Odometry</article-title>
                  <conf-name>Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognitio</conf-name>
                  <conf-loc>Washington, DC, USA</conf-loc>
                  <conf-date>27 June–2 July 2004</conf-date>
                  <fpage>652</fpage>
                  <lpage>659</lpage>
                </element-citation>
              </ref>
              <ref id="b6-sensors-14-04981">
                <label>6.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Scaramuzza</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Fraundorfer</surname>
                      <given-names>F.</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual odometry [tutorial]</article-title>
                  <source>IEEE Rob. Autom. Mag.</source>
                  <year>2011</year>
                  <volume>18</volume>
                  <fpage>80</fpage>
                  <lpage>92</lpage>
                </element-citation>
              </ref>
              <ref id="b7-sensors-14-04981">
                <label>7.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nistér</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Naroditsky</surname>
                      <given-names>O.</given-names>
                    </name>
                    <name>
                      <surname>Bergen</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual odometry for ground vehicle applications</article-title>
                  <source>J. Field. Rob.</source>
                  <year>2006</year>
                  <volume>23</volume>
                  <fpage>3</fpage>
                  <lpage>20</lpage>
                </element-citation>
              </ref>
              <ref id="b8-sensors-14-04981">
                <label>8.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Scaramuzza</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>1-point-ransac structure from motion for vehicle-mounted cameras by exploiting non-holonomic constraints</article-title>
                  <source>Int. J. Comput. Vision</source>
                  <year>2011</year>
                  <volume>95</volume>
                  <fpage>74</fpage>
                  <lpage>85</lpage>
                </element-citation>
              </ref>
              <ref id="b9-sensors-14-04981">
                <label>9.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Davison</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Real-Time Simultaneous Localisation and Mapping with a Single Camera</article-title>
                  <conf-name>Proceedings of the 9th IEEE International Conference on Computer Vision</conf-name>
                  <conf-loc>Nice, France</conf-loc>
                  <conf-date>14–17 October 2003</conf-date>
                  <fpage>1403</fpage>
                  <lpage>1410</lpage>
                </element-citation>
              </ref>
              <ref id="b10-sensors-14-04981">
                <label>10.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Klein</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Murray</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Parallel Tracking and Mapping for Small AR Workspace<italic>s</italic></article-title>
                  <conf-name>Proceedings of the 6th IEEE and ACM International Symposium on Mixed and Augmented Reality</conf-name>
                  <conf-loc>Nara, Japan</conf-loc>
                  <conf-date>13–16 November 2007</conf-date>
                  <fpage>225</fpage>
                  <lpage>234</lpage>
                </element-citation>
              </ref>
              <ref id="b11-sensors-14-04981">
                <label>11.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Mouragnon</surname>
                      <given-names>E.</given-names>
                    </name>
                    <name>
                      <surname>Lhuillier</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Dhome</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Dekeyser</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Sayd</surname>
                      <given-names>P.</given-names>
                    </name>
                  </person-group>
                  <article-title>Real time localization and 3D reconstruction</article-title>
                  <conf-name>Proceedings of 2006IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name>
                  <conf-loc>New York, NY, USA</conf-loc>
                  <conf-date>17–22 June 2006</conf-date>
                  <fpage>363</fpage>
                  <lpage>370</lpage>
                </element-citation>
              </ref>
              <ref id="b12-sensors-14-04981">
                <label>12.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Strasdat</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Montiel</surname>
                      <given-names>J.M.M.</given-names>
                    </name>
                    <name>
                      <surname>Davison</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>Scale Drift-Aware Large Scale Monocular SLAM</article-title>
                  <source>Rob. Sci. Syst.</source>
                  <year>2010</year>
                  <volume>2</volume>
                  <fpage>5</fpage>
                  <lpage>8</lpage>
                </element-citation>
              </ref>
              <ref id="b13-sensors-14-04981">
                <label>13.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Triggs</surname>
                      <given-names>B.</given-names>
                    </name>
                    <name>
                      <surname>McLauchlan</surname>
                      <given-names>P.F.</given-names>
                    </name>
                    <name>
                      <surname>Hartley</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Fitzgibbon</surname>
                      <given-names>A.W.</given-names>
                    </name>
                  </person-group>
                  <source>Bundle Adjustment—A Modern Synthesis. Vision Algorithms: Theory and Practice</source>
                  <publisher-name>Springer-Verlag</publisher-name>
                  <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
                  <year>2000</year>
                  <fpage>298</fpage>
                  <lpage>372</lpage>
                </element-citation>
              </ref>
              <ref id="b14-sensors-14-04981">
                <label>14.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Scaramuzza</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Siegwart</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicles</article-title>
                  <source>IEEE Trans. Rob.</source>
                  <year>2008</year>
                  <volume>24</volume>
                  <fpage>1015</fpage>
                  <lpage>1026</lpage>
                </element-citation>
              </ref>
              <ref id="b15-sensors-14-04981">
                <label>15.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Scaramuzza</surname>
                      <given-names>D.</given-names>
                    </name>
                    <name>
                      <surname>Fraundorfer</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Siegwart</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <article-title>Real-time monocular visual odometry for on-road vehicles with 1-point RANSAC</article-title>
                  <conf-name>Proceedings of the 2009 IEEE Computer Society Conference on Robotics and Automation</conf-name>
                  <conf-loc>Kobe, Japan</conf-loc>
                  <conf-date>12–17 May 2009</conf-date>
                  <fpage>4293</fpage>
                  <lpage>4299</lpage>
                </element-citation>
              </ref>
              <ref id="b16-sensors-14-04981">
                <label>16.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Konolige</surname>
                      <given-names>K.</given-names>
                    </name>
                    <name>
                      <surname>Agrawal</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Sola</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Large-scale visual odometry for rough terrain</article-title>
                  <source>Robotics Research</source>
                  <person-group person-group-type="editor">
                    <name>
                      <surname>Kaneko</surname>
                      <given-names>M.</given-names>
                    </name>
                    <name>
                      <surname>Nakamura</surname>
                      <given-names>Y.</given-names>
                    </name>
                  </person-group>
                  <publisher-name>Springer-Verlag</publisher-name>
                  <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
                  <year>2011</year>
                  <fpage>201</fpage>
                  <lpage>212</lpage>
                </element-citation>
              </ref>
              <ref id="b17-sensors-14-04981">
                <label>17.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chu</surname>
                      <given-names>T.</given-names>
                    </name>
                    <name>
                      <surname>Guo</surname>
                      <given-names>N.</given-names>
                    </name>
                    <name>
                      <surname>Backén</surname>
                      <given-names>S.</given-names>
                    </name>
                    <name>
                      <surname>Akos</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Monocular camera/IMU/GNSS integration for ground vehicle navigation in challenging GNSS environments</article-title>
                  <source>Sensors</source>
                  <year>2012</year>
                  <volume>12</volume>
                  <fpage>3162</fpage>
                  <lpage>3185</lpage>
                  <pub-id pub-id-type="pmid">22736999</pub-id>
                </element-citation>
              </ref>
              <ref id="b18-sensors-14-04981">
                <label>18.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Zhang</surname>
                      <given-names>X.</given-names>
                    </name>
                    <name>
                      <surname>Rad</surname>
                      <given-names>A.</given-names>
                    </name>
                    <name>
                      <surname>Wong</surname>
                      <given-names>Y.</given-names>
                    </name>
                  </person-group>
                  <article-title>Sensor fusion of monocular cameras and laser rangefinders for line-based simultaneous localization and mapping (SLAM) tasks in autonomous mobile robots</article-title>
                  <source>Sensors</source>
                  <year>2012</year>
                  <volume>12</volume>
                  <fpage>429</fpage>
                  <lpage>452</lpage>
                  <pub-id pub-id-type="pmid">22368478</pub-id>
                </element-citation>
              </ref>
              <ref id="b19-sensors-14-04981">
                <label>19.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Ordonez</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Arias</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Herráez</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Rodriguez</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Martín</surname>
                      <given-names>M.T.</given-names>
                    </name>
                  </person-group>
                  <article-title>A combined single range and single image device for low—Cost measurement of building façade features</article-title>
                  <source>Photogramm. Rec.</source>
                  <year>2008</year>
                  <volume>23</volume>
                  <fpage>228</fpage>
                  <lpage>240</lpage>
                </element-citation>
              </ref>
              <ref id="b20-sensors-14-04981">
                <label>20.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Martínez</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Ordonez</surname>
                      <given-names>C.</given-names>
                    </name>
                    <name>
                      <surname>Arias</surname>
                      <given-names>P.</given-names>
                    </name>
                    <name>
                      <surname>Armesto</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Non-contact 3D Measurement of Buildings through Close Range Photogrammetry and a Laser Distance Meter</article-title>
                  <source>Photogramm. Eng. Remote Sens.</source>
                  <year>2011</year>
                  <volume>77</volume>
                  <fpage>805</fpage>
                  <lpage>811</lpage>
                </element-citation>
              </ref>
              <ref id="b21-sensors-14-04981">
                <label>21.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Fraundorfer</surname>
                      <given-names>F.</given-names>
                    </name>
                    <name>
                      <surname>Scaramuzza</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>Visual Odometry: Part II: Matching, Robustness, Optimization, and Applications</article-title>
                  <source>IEEE Rob. Autom. Mag.</source>
                  <year>2012</year>
                  <volume>19</volume>
                  <fpage>78</fpage>
                  <lpage>90</lpage>
                </element-citation>
              </ref>
              <ref id="b22-sensors-14-04981">
                <label>22.</label>
                <element-citation publication-type="webpage">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bouguet</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Camera Calibration Toolbox for Matlab</article-title>
                  <year>2008</year>
                  <comment>Available online <ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/bouguetj/calib">http://www.vision.caltech.edu/bouguetj/calib doc/</ext-link></comment>
                  <date-in-citation>(accessed on 20 October 2013)</date-in-citation>
                </element-citation>
              </ref>
              <ref id="b23-sensors-14-04981">
                <label>23.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Otsu</surname>
                      <given-names>N.</given-names>
                    </name>
                  </person-group>
                  <article-title>A threshold selection method from gray-level histograms</article-title>
                  <source>Automatica</source>
                  <year>1979</year>
                  <volume>11</volume>
                  <fpage>23</fpage>
                  <lpage>27</lpage>
                </element-citation>
              </ref>
              <ref id="b24-sensors-14-04981">
                <label>24.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Corke</surname>
                      <given-names>P.</given-names>
                    </name>
                  </person-group>
                  <source>Robotics, Vision and Control: Fundamental Algorithms in MATLAB</source>
                  <publisher-name>Springer-Verlag</publisher-name>
                  <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
                  <year>2011</year>
                  <fpage>351</fpage>
                  <lpage>355</lpage>
                </element-citation>
              </ref>
              <ref id="b25-sensors-14-04981">
                <label>25.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Torr</surname>
                      <given-names>P.H.S.</given-names>
                    </name>
                    <name>
                      <surname>Zisserman</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <article-title>MLESAC: A new robust estimator with application to estimating image geometry</article-title>
                  <source>Comput. Vision Image Underst.</source>
                  <year>2000</year>
                  <volume>78</volume>
                  <fpage>138</fpage>
                  <lpage>156</lpage>
                </element-citation>
              </ref>
              <ref id="b26-sensors-14-04981">
                <label>26.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hartley</surname>
                      <given-names>R.</given-names>
                    </name>
                    <name>
                      <surname>Zisserman</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <source>Multiple View Geometry in Computer Vision</source>
                  <edition>2nd ed.</edition>
                  <publisher-name>Cambridge University Press</publisher-name>
                  <publisher-loc>Cambridge, UK</publisher-loc>
                  <year>2004</year>
                </element-citation>
              </ref>
              <ref id="b27-sensors-14-04981">
                <label>27.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Chum</surname>
                      <given-names>O.</given-names>
                    </name>
                    <name>
                      <surname>Matas</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Matching with PROSAC-progressive sample consensus</article-title>
                  <conf-name>Proceedings of the 2005IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005</conf-name>
                  <conf-loc>San Diego, CA, USA</conf-loc>
                  <conf-date>20–25 June 2005</conf-date>
                  <fpage>pp: 220</fpage>
                  <lpage>226</lpage>
                </element-citation>
              </ref>
              <ref id="b28-sensors-14-04981">
                <label>28.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Shi</surname>
                      <given-names>J.</given-names>
                    </name>
                    <name>
                      <surname>Tomasi</surname>
                      <given-names>C.</given-names>
                    </name>
                  </person-group>
                  <article-title>Good Features to Track</article-title>
                  <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>
                  <conf-loc>Seattle, WA, USA</conf-loc>
                  <conf-date>21–23 June 1994</conf-date>
                  <fpage>593</fpage>
                  <lpage>600</lpage>
                </element-citation>
              </ref>
              <ref id="b29-sensors-14-04981">
                <label>29.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Bradski</surname>
                      <given-names>G.</given-names>
                    </name>
                    <name>
                      <surname>Kaehler</surname>
                      <given-names>A.</given-names>
                    </name>
                  </person-group>
                  <source>Learning OpenCV: Computer Vision with the OpenCV Library</source>
                  <publisher-name>O'Reilly Media</publisher-name>
                  <publisher-loc>Sebastopol, CA, USA</publisher-loc>
                  <year>2008</year>
                </element-citation>
              </ref>
              <ref id="b30-sensors-14-04981">
                <label>30.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lucchese</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Mitra</surname>
                      <given-names>S.K.</given-names>
                    </name>
                  </person-group>
                  <article-title>Using saddle points for sub-pixel feature detection in camera calibration targets</article-title>
                  <conf-name>Proceedings of the 2002 Asia Pacific Conference on Circuits and Systems</conf-name>
                  <conf-loc>Kaohsiung, Taiwan</conf-loc>
                  <conf-date>2–5 December 2012</conf-date>
                  <fpage>191</fpage>
                  <lpage>195</lpage>
                </element-citation>
              </ref>
              <ref id="b31-sensors-14-04981">
                <label>31.</label>
                <element-citation publication-type="confproc">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Lucas</surname>
                      <given-names>B.D.</given-names>
                    </name>
                    <name>
                      <surname>Kanade</surname>
                      <given-names>T.</given-names>
                    </name>
                  </person-group>
                  <article-title>An iterative image registration technique with an application to stereo vision</article-title>
                  <conf-name>Proceedings of the 7th International Joint Conference on Artificial Intelligence</conf-name>
                  <conf-loc>Vancouver, BC, Canada</conf-loc>
                  <conf-date>24–28 August 1981</conf-date>
                  <fpage>674</fpage>
                  <lpage>679</lpage>
                </element-citation>
              </ref>
              <ref id="b32-sensors-14-04981">
                <label>32.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Szeliski</surname>
                      <given-names>R.</given-names>
                    </name>
                  </person-group>
                  <source>Computer Vision: Algorithms and Applications</source>
                  <publisher-name>Springer-Verlag</publisher-name>
                  <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
                  <year>2011</year>
                  <fpage>232</fpage>
                  <lpage>234</lpage>
                </element-citation>
              </ref>
              <ref id="b33-sensors-14-04981">
                <label>33.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Nistér</surname>
                      <given-names>D.</given-names>
                    </name>
                  </person-group>
                  <article-title>An efficient solution to the five-point relative pose problem</article-title>
                  <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
                  <year>2004</year>
                  <volume>26</volume>
                  <fpage>756</fpage>
                  <lpage>770</lpage>
                  <pub-id pub-id-type="pmid">18579936</pub-id>
                </element-citation>
              </ref>
              <ref id="b34-sensors-14-04981">
                <label>34.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Horn</surname>
                      <given-names>B.K.</given-names>
                    </name>
                  </person-group>
                  <article-title>Recovering baseline and orientation from essential matrix</article-title>
                  <source>J. Opt. Soc. Am. A.</source>
                  <year>1990</year>
                  <volume>1</volume>
                  <fpage>1</fpage>
                  <lpage>10</lpage>
                </element-citation>
              </ref>
              <ref id="b35-sensors-14-04981">
                <label>35.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Horn</surname>
                      <given-names>B.K.</given-names>
                    </name>
                    <name>
                      <surname>Schunck</surname>
                      <given-names>B.G.</given-names>
                    </name>
                  </person-group>
                  <article-title>Determining optical flow</article-title>
                  <source>Artif. Intell.</source>
                  <year>1981</year>
                  <volume>17</volume>
                  <fpage>185</fpage>
                  <lpage>203</lpage>
                </element-citation>
              </ref>
              <ref id="b36-sensors-14-04981">
                <label>36.</label>
                <element-citation publication-type="book">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Esteban</surname>
                      <given-names>I.</given-names>
                    </name>
                    <name>
                      <surname>Dorst</surname>
                      <given-names>L.</given-names>
                    </name>
                    <name>
                      <surname>Dijk</surname>
                      <given-names>J.</given-names>
                    </name>
                  </person-group>
                  <article-title>Closed form Solution for the Scale Ambiguity Problem in Monocular Visual Odometry</article-title>
                  <source>Intelligent Robotics and Application</source>
                  <person-group person-group-type="editor">
                    <name>
                      <surname>Liu</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Ding</surname>
                      <given-names>H.</given-names>
                    </name>
                    <name>
                      <surname>Xiong</surname>
                      <given-names>Z.</given-names>
                    </name>
                    <name>
                      <surname>Zhu</surname>
                      <given-names>X.</given-names>
                    </name>
                  </person-group>
                  <publisher-name>Springer-Verlag</publisher-name>
                  <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
                  <year>2010</year>
                  <fpage>665</fpage>
                  <lpage>679</lpage>
                </element-citation>
              </ref>
              <ref id="b37-sensors-14-04981">
                <label>37.</label>
                <element-citation publication-type="journal">
                  <person-group person-group-type="author">
                    <name>
                      <surname>Hansen</surname>
                      <given-names>J.E.</given-names>
                    </name>
                  </person-group>
                  <article-title>Exact and approximate solutions for multiple scattering by cloudy and hazy planetary atmospheres</article-title>
                  <source>J. Atmos. Sci.</source>
                  <year>1969</year>
                  <volume>26</volume>
                  <fpage>478</fpage>
                  <lpage>487</lpage>
                </element-citation>
              </ref>
            </ref-list>
          </back>
          <floats-group>
            <fig id="f1-sensors-14-04981" position="float">
              <label>Figure 1.</label>
              <caption>
                <p>The hardware scheme for the astronaut navigation system.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f1"/>
            </fig>
            <fig id="f2-sensors-14-04981" position="float">
              <label>Figure 2.</label>
              <caption>
                <p>Geometrical relationship of the LDM and the camera.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f2"/>
            </fig>
            <fig id="f3-sensors-14-04981" position="float">
              <label>Figure 3.</label>
              <caption>
                <p>The enhanced scale drift free monocular VO system.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f3"/>
            </fig>
            <fig id="f4-sensors-14-04981" position="float">
              <label>Figure 4.</label>
              <caption>
                <p>Flowchart of the enhanced monocular VO system pipeline.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f4"/>
            </fig>
            <fig id="f5-sensors-14-04981" position="float">
              <label>Figure 5.</label>
              <caption>
                <p>(<bold>a</bold>) Enlarged picture around the detected center at a distance of 5 m from the wall; (<bold>b</bold>) Enlarged picture around the detected center at a distance of 0.5 m from the wall.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f5"/>
            </fig>
            <fig id="f6-sensors-14-04981" position="float">
              <label>Figure 6.</label>
              <caption>
                <p>The image position of the projection center with varying laser distance measurements.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f6"/>
            </fig>
            <fig id="f7-sensors-14-04981" position="float">
              <label>Figure 7.</label>
              <caption>
                <p>(<bold>a</bold>) Laser pointer projections on an image; (<bold>b</bold>) Laser pointer projections on an image with gross error removed after MLESAC.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f7"/>
            </fig>
            <fig id="f8-sensors-14-04981" position="float">
              <label>Figure 8.</label>
              <caption>
                <p>Laser pointer projection on the estimated line.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f8"/>
            </fig>
            <fig id="f9-sensors-14-04981" position="float">
              <label>Figure 9.</label>
              <caption>
                <p>The combined calibration setup.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f9"/>
            </fig>
            <fig id="f10-sensors-14-04981" position="float">
              <label>Figure 10.</label>
              <caption>
                <p>(<bold>a</bold>) Original key-frame selected from the dataset; (<bold>b</bold>) Feature tracking between nearby key-frames (previous dotted in green, current dotted in blue), stereo matches (green), and triple matches (red).</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f10"/>
            </fig>
            <fig id="f11-sensors-14-04981" position="float">
              <label>Figure 11.</label>
              <caption>
                <p>The laser pointer projection (right image, marked in red) is matched on the key-frame (marked in red) through disparity consistency constraints by Delaunay triangulation (blue), checked by the laser pointer's conjugate epipolar line (green).</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f11"/>
            </fig>
            <fig id="f12-sensors-14-04981" position="float">
              <label>Figure 12.</label>
              <caption>
                <p>(<bold>a</bold>) Sample images from the first dataset at a construction site; (<bold>b</bold>) Sample images from the second dataset in a desert.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f12"/>
            </fig>
            <fig id="f13-sensors-14-04981" position="float">
              <label>Figure 13.</label>
              <caption>
                <p>Estimated trajectory using monocular VO (blue); estimated trajectory using enhanced monocular VO (green); red dots represent frames when the synchronized laser pointer was successfully matched and scale correction was taken. (<bold>a</bold>) the first dataset; (<bold>b</bold>) the second dataset.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f13"/>
            </fig>
            <fig id="f14-sensors-14-04981" position="float">
              <label>Figure 14.</label>
              <caption>
                <p>The distance error of the laser pointer with our enhanced monocular VO scheme compared to the traditional one. (<bold>a</bold>) the first dataset; (<bold>b</bold>) the second dataset.</p>
              </caption>
              <graphic xlink:href="sensors-14-04981f14"/>
            </fig>
          </floats-group>
        </article>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
